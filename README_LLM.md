# ü§ñ G√©n√©ration de Lettres de Motivation avec LLM

Ce syst√®me permet de g√©n√©rer des lettres de motivation personnalis√©es et coh√©rentes en utilisant des mod√®les de langage (LLM).

## üéØ Options disponibles

### 1. **Ollama (Recommand√© - Gratuit et Local)** ‚≠ê

**Avantages :**
- ‚úÖ Gratuit
- ‚úÖ Fonctionne localement (pas de cl√© API)
- ‚úÖ Donn√©es priv√©es (rien n'est envoy√© sur internet)
- ‚úÖ Plusieurs mod√®les disponibles

**Installation :**

1. **Installer Ollama :**
   ```bash
   # macOS
   brew install ollama
   # ou t√©l√©chargez depuis https://ollama.ai
   ```

2. **Lancer Ollama :**
   ```bash
   ollama serve
   ```

3. **T√©l√©charger un mod√®le :**
   ```bash
   # Mod√®les recommand√©s (choisissez-en un) :
   ollama pull llama3.2        # L√©ger et rapide (2GB)
   ollama pull mistral         # Bon compromis (4GB)
   ollama pull qwen2.5         # Excellent pour le fran√ßais (7GB)
   ollama pull llama3.1        # Plus performant (4.7GB)
   ```

4. **Configuration :**
   Cr√©ez un fichier `.env` √† la racine du projet :
   ```env
   LLM_PROVIDER=ollama
   LLM_MODEL=llama3.2  # ou le mod√®le que vous avez t√©l√©charg√©
   ```

### 2. **OpenAI API (Payant mais tr√®s performant)**

**Avantages :**
- ‚úÖ Excellent pour le fran√ßais
- ‚úÖ Tr√®s rapide
- ‚úÖ Mod√®les GPT-4 disponibles

**Installation :**

1. **Installer le package :**
   ```bash
   pip install openai
   ```

2. **Obtenir une cl√© API :**
   - Allez sur https://platform.openai.com
   - Cr√©ez un compte
   - G√©n√©rez une cl√© API

3. **Configuration :**
   Dans votre fichier `.env` :
   ```env
   LLM_PROVIDER=openai
   LLM_MODEL=gpt-4o-mini  # ou gpt-4, gpt-3.5-turbo
   OPENAI_API_KEY=votre_cle_api_ici
   ```

**Co√ªts approximatifs :**
- GPT-4o-mini : ~$0.15 / 1M tokens (tr√®s √©conomique)
- GPT-4 : ~$30 / 1M tokens (plus cher mais meilleur)

### 3. **Mistral AI (Bon compromis prix/performance)**

**Avantages :**
- ‚úÖ Bon rapport qualit√©/prix
- ‚úÖ Excellent pour le fran√ßais
- ‚úÖ API europ√©enne

**Installation :**

1. **Installer le package :**
   ```bash
   pip install mistralai
   ```

2. **Obtenir une cl√© API :**
   - Allez sur https://console.mistral.ai
   - Cr√©ez un compte
   - G√©n√©rez une cl√© API

3. **Configuration :**
   Dans votre fichier `.env` :
   ```env
   LLM_PROVIDER=mistral
   LLM_MODEL=mistral-medium  # ou mistral-small, mistral-large
   MISTRAL_API_KEY=votre_cle_api_ici
   ```

**Co√ªts approximatifs :**
- Mistral-small : ~$0.20 / 1M tokens
- Mistral-medium : ~$2.70 / 1M tokens

### 4. **Anthropic Claude (Excellent pour le fran√ßais)**

**Avantages :**
- ‚úÖ Excellent pour le fran√ßais
- ‚úÖ Tr√®s bon contexte
- ‚úÖ Mod√®les tr√®s performants

**Installation :**

1. **Installer le package :**
   ```bash
   pip install anthropic
   ```

2. **Obtenir une cl√© API :**
   - Allez sur https://console.anthropic.com
   - Cr√©ez un compte
   - G√©n√©rez une cl√© API

3. **Configuration :**
   Dans votre fichier `.env` :
   ```env
   LLM_PROVIDER=claude
   LLM_MODEL=claude-3-5-sonnet-20241022  # ou claude-3-opus, claude-3-haiku
   ANTHROPIC_API_KEY=votre_cle_api_ici
   ```

**Co√ªts approximatifs :**
- Claude-3 Haiku : ~$0.25 / 1M tokens
- Claude-3 Sonnet : ~$3 / 1M tokens
- Claude-3 Opus : ~$15 / 1M tokens

## üöÄ Utilisation

### Configuration automatique

Le syst√®me d√©tecte automatiquement votre configuration depuis le fichier `.env` ou utilise Ollama par d√©faut.

### Dans le code

```python
from cover_letter_generator import CoverLetterGenerator

# Utiliser le LLM (recommand√©)
generator = CoverLetterGenerator(use_llm=True)

# Ou sp√©cifier un provider
generator = CoverLetterGenerator(
    use_llm=True,
    llm_provider="ollama",  # ou "openai", "mistral", "claude"
    llm_model="llama3.2"
)

# G√©n√©rer une lettre
letter = generator.generate_cover_letter(job, personal_info)
```

### Dans l'application Streamlit

L'application utilise automatiquement le LLM si configur√©. Sinon, elle utilise les templates de secours.

## üìù Exemple de fichier .env

```env
# Choix du provider LLM
LLM_PROVIDER=ollama
LLM_MODEL=llama3.2

# Si vous utilisez OpenAI
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4o-mini
# OPENAI_API_KEY=sk-...

# Si vous utilisez Mistral
# LLM_PROVIDER=mistral
# LLM_MODEL=mistral-medium
# MISTRAL_API_KEY=...

# Si vous utilisez Claude
# LLM_PROVIDER=claude
# LLM_MODEL=claude-3-5-sonnet-20241022
# ANTHROPIC_API_KEY=sk-ant-...
```

## üîß D√©pannage

### Ollama ne r√©pond pas

1. V√©rifiez que Ollama est lanc√© : `ollama serve`
2. V√©rifiez que le mod√®le est t√©l√©charg√© : `ollama list`
3. Testez manuellement : `ollama run llama3.2`

### Erreur de cl√© API

- V√©rifiez que votre cl√© API est correcte dans `.env`
- V√©rifiez que le package correspondant est install√© (`pip install openai`, etc.)

### Lettre g√©n√©r√©e incompl√®te

- Le syst√®me utilise automatiquement les templates de secours si le LLM √©choue
- V√©rifiez les logs pour voir l'erreur exacte

## üí° Recommandations

1. **Pour commencer :** Utilisez **Ollama** (gratuit, local, pas de cl√© API)
2. **Pour la qualit√© maximale :** Utilisez **OpenAI GPT-4** ou **Claude Opus**
3. **Pour un bon compromis :** Utilisez **Mistral Medium** ou **Claude Sonnet**

## üìä Comparaison rapide

| Provider | Co√ªt | Qualit√© | Vitesse | Confidentialit√© |
|----------|------|---------|---------|-----------------|
| Ollama | Gratuit | Bonne | Moyenne | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (local) |
| OpenAI | Payant | Excellente | Rapide | ‚≠ê‚≠ê‚≠ê (API) |
| Mistral | Payant | Tr√®s bonne | Rapide | ‚≠ê‚≠ê‚≠ê (API) |
| Claude | Payant | Excellente | Rapide | ‚≠ê‚≠ê‚≠ê (API) |

## üéØ Mod√®les recommand√©s par provider

- **Ollama :** `llama3.2`, `mistral`, `qwen2.5`
- **OpenAI :** `gpt-4o-mini` (√©conomique), `gpt-4` (meilleur)
- **Mistral :** `mistral-small` (√©conomique), `mistral-medium` (√©quilibr√©)
- **Claude :** `claude-3-haiku` (√©conomique), `claude-3-5-sonnet` (√©quilibr√©)


