name: Daily Job Scraping

on:
  schedule:
    # Exécute tous les jours à 10h00 UTC (11h00 heure française en hiver, 12h00 en été)
    - cron: '0 10 * * *'
  workflow_dispatch:  # Permet de déclencher manuellement depuis GitHub

jobs:
  scrape-and-notify:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.13'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y wget gnupg
        wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
        CHROME_VERSION=$(google-chrome --version | cut -d " " -f3 | cut -d "." -f1)
        CHROMEDRIVER_VERSION=$(curl -s "https://chromedriver.storage.googleapis.com/LATEST_RELEASE_${CHROME_VERSION}")
        wget -q "https://chromedriver.storage.googleapis.com/${CHROMEDRIVER_VERSION}/chromedriver_linux64.zip"
        unzip chromedriver_linux64.zip
        sudo mv chromedriver /usr/local/bin/chromedriver
        sudo chmod +x /usr/local/bin/chromedriver
        rm chromedriver_linux64.zip
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt || pip install --no-cache-dir -r requirements.txt
        # Vérifier les installations critiques
        python -c "import selenium; print('Selenium OK')" || echo "Selenium manquant"
        python -c "import bs4; print('BeautifulSoup OK')" || echo "BeautifulSoup manquant"
    
    - name: Set up Chrome for Selenium
      run: |
        export CHROME_BIN=/usr/bin/google-chrome
        export CHROMEDRIVER_PATH=/usr/local/bin/chromedriver
        echo "CHROME_BIN=$CHROME_BIN" >> $GITHUB_ENV
        echo "CHROMEDRIVER_PATH=$CHROMEDRIVER_PATH" >> $GITHUB_ENV
        # Vérifier que Chrome est installé
        google-chrome --version || echo "Chrome non trouvé"
        chromedriver --version || echo "ChromeDriver non trouvé"
    
    - name: Create data directory
      run: |
        mkdir -p data
        touch data/.gitkeep
    
    - name: Run daily scraping
      env:
        LINKEDIN_EMAIL: ${{ secrets.secret1 }}
        LINKEDIN_PASSWORD: ${{ secrets.secret2 }}
        EMAIL_SMTP_SERVER: ${{ secrets.secret3 }}
        EMAIL_SMTP_PORT: ${{ secrets.secret4 }}
        EMAIL_SMTP_USER: ${{ secrets.secret5 }}
        EMAIL_SMTP_PASSWORD: ${{ secrets.secret6 }}
        EMAIL_SENDER: ${{ secrets.secret7 }}
        EMAIL_RECIPIENT: ${{ secrets.secret8 }}
        GITHUB_ACTIONS: 'true'
      run: |
        python scrape_jobs_airflow.py || echo "Scraping échoué mais on continue"
    
    - name: Compare jobs and send email
      env:
        EMAIL_SMTP_SERVER: ${{ secrets.secret3 }}
        EMAIL_SMTP_PORT: ${{ secrets.secret4 }}
        EMAIL_SMTP_USER: ${{ secrets.secret5 }}
        EMAIL_SMTP_PASSWORD: ${{ secrets.secret6 }}
        EMAIL_SENDER: ${{ secrets.secret7 }}
        EMAIL_RECIPIENT: ${{ secrets.secret8 }}
      run: |
        python -c "from compare_jobs import get_new_jobs; from email_notifier import send_email; new_jobs, all_jobs = get_new_jobs(); send_email(new_jobs, len(all_jobs))"
    
    - name: Upload results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: job-results
        path: |
          data/jobs.json
          data/jobs_previous.json
        retention-days: 30

# Workflow activé
