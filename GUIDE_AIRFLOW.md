# Guide d'installation et configuration d'Airflow

Ce guide vous explique comment configurer Apache Airflow pour automatiser le scraping quotidien des offres d'emploi et recevoir des alertes par email.

## üìã Pr√©requis

- Python 3.8+ (vous avez d√©j√† Python 3.13)
- Un compte email (Gmail recommand√© pour la simplicit√©)
- Un environnement virtuel Python (d√©j√† cr√©√© : `venv`)

## üöÄ Installation d'Airflow

### Option 1 : Installation dans l'environnement virtuel existant (recommand√©)

```bash
# Activer l'environnement virtuel
source venv/bin/activate

# Installer Airflow
pip install apache-airflow==2.7.0

# Ou pour une installation plus l√©g√®re (sans providers suppl√©mentaires)
pip install apache-airflow==2.7.0 --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.7.0/constraints-3.13.txt"
```

### Option 2 : Installation avec Docker (plus simple mais n√©cessite Docker)

Si vous pr√©f√©rez Docker, vous pouvez utiliser l'image officielle d'Airflow.

## ‚öôÔ∏è Configuration

### 1. Configuration de l'email

√âditez votre fichier `.env` (ou cr√©ez-le s'il n'existe pas) :

```bash
# Configuration Email pour les alertes
EMAIL_SMTP_SERVER=smtp.gmail.com
EMAIL_SMTP_PORT=587
EMAIL_SMTP_USER=votre.email@gmail.com
EMAIL_SMTP_PASSWORD=votre_mot_de_passe_application
EMAIL_SENDER=votre.email@gmail.com
EMAIL_RECIPIENT=votre.email@gmail.com
```

**Important pour Gmail :**
- Vous devez utiliser un **mot de passe d'application** (pas votre mot de passe Gmail normal)
- Pour cr√©er un mot de passe d'application :
  1. Allez sur https://myaccount.google.com/security
  2. Activez la validation en 2 √©tapes si ce n'est pas d√©j√† fait
  3. Allez dans "Mots de passe des applications"
  4. Cr√©ez un nouveau mot de passe d'application pour "Mail"
  5. Utilisez ce mot de passe dans `EMAIL_SMTP_PASSWORD`

**Pour d'autres fournisseurs email :**
- **Outlook/Hotmail** : `smtp-mail.outlook.com`, port 587
- **Yahoo** : `smtp.mail.yahoo.com`, port 587
- **Autres** : Consultez la documentation de votre fournisseur

### 2. Initialisation d'Airflow

```bash
# Cr√©er le r√©pertoire AIRFLOW_HOME (si pas d√©j√† fait)
export AIRFLOW_HOME=~/airflow

# Initialiser la base de donn√©es
airflow db init

# Cr√©er un utilisateur admin
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email votre.email@gmail.com \
    --password admin
```

### 3. Configuration du DAG

Le DAG est d√©j√† cr√©√© dans `dags/job_scraper_dag.py`. Assurez-vous que :

1. Le chemin vers votre projet est correct dans le DAG
2. Les variables d'environnement sont charg√©es (via `.env`)

### 4. Tester le scraping manuellement

Avant de lancer Airflow, testez que tout fonctionne :

```bash
# Activer l'environnement virtuel
source venv/bin/activate

# Tester le scraping
python scrape_jobs_airflow.py

# Tester la comparaison et l'email
python -c "from compare_jobs import get_new_jobs; from email_notifier import send_email; new_jobs, all_jobs = get_new_jobs(); send_email(new_jobs, len(all_jobs))"
```

## üéØ Lancement d'Airflow

### 1. D√©marrer le scheduler et le webserver

Dans un premier terminal :

```bash
source venv/bin/activate
export AIRFLOW_HOME=~/airflow
airflow webserver --port 8080
```

Dans un deuxi√®me terminal :

```bash
source venv/bin/activate
export AIRFLOW_HOME=~/airflow
airflow scheduler
```

### 2. Acc√©der √† l'interface web

Ouvrez votre navigateur et allez sur : http://localhost:8080

- **Username** : admin
- **Password** : admin (ou celui que vous avez d√©fini)

### 3. Activer le DAG

1. Dans l'interface Airflow, cherchez le DAG `daily_job_scraper`
2. Activez-le en cliquant sur le toggle √† gauche
3. Le DAG s'ex√©cutera automatiquement tous les jours √† 10h00

### 4. Tester manuellement le DAG

Pour tester imm√©diatement sans attendre 10h :

1. Cliquez sur le DAG `daily_job_scraper`
2. Cliquez sur le bouton "Play" (‚ñ∂Ô∏è) en haut √† droite
3. S√©lectionnez "Trigger DAG"

## üìß Format des emails

Les emails contiendront :
- Le nombre total d'offres
- Le nombre de nouvelles offres d√©tect√©es
- La liste d√©taill√©e des nouvelles offres avec :
  - Titre du poste
  - Entreprise
  - Localisation
  - Source (LinkedIn, Indeed, etc.)
  - Lien vers l'offre

## üîß Personnalisation

### Changer l'heure d'ex√©cution

√âditez `dags/job_scraper_dag.py` et modifiez la ligne :

```python
schedule_interval='0 10 * * *',  # Tous les jours √† 10h00
```

Format cron : `minute heure jour mois jour_semaine`
- `0 10 * * *` = 10h00 tous les jours
- `0 8 * * 1-5` = 8h00 du lundi au vendredi
- `0 */6 * * *` = Toutes les 6 heures

### Changer les param√®tres de recherche

√âditez `scrape_jobs_airflow.py` et modifiez :

```python
keywords = "Data"  # Vos mots-cl√©s
location = config.DEFAULT_LOCATION  # Votre localisation
pages = 2  # Nombre de pages par site
```

### D√©sactiver l'envoi d'email si aucune nouvelle offre

√âditez `dags/job_scraper_dag.py` dans la fonction `compare_and_notify_task` :

```python
# Ne pas envoyer d'email s'il n'y a pas de nouvelles offres
if new_jobs_count == 0:
    print_info("Aucune nouvelle offre, pas d'email envoy√©")
    return {'status': 'success', 'new_jobs': 0, 'email_sent': False}
```

## üêõ D√©pannage

### Le DAG n'appara√Æt pas

1. V√©rifiez que le fichier `dags/job_scraper_dag.py` existe
2. V√©rifiez qu'il n'y a pas d'erreurs de syntaxe : `python -m py_compile dags/job_scraper_dag.py`
3. Red√©marrez le scheduler : `airflow scheduler`

### Erreur d'import des modules

Assurez-vous que le chemin dans `dags/job_scraper_dag.py` est correct :

```python
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
```

### Erreur d'envoi d'email

1. V√©rifiez vos identifiants dans `.env`
2. Pour Gmail, utilisez un mot de passe d'application
3. Testez manuellement : `python email_notifier.py`

### Le scraping √©choue

1. V√©rifiez que Selenium fonctionne : `python main_unified.py --search "Data" --location "Haute-Garonne, France" --pages 1`
2. V√©rifiez les logs dans l'interface Airflow

## üìù Logs

Les logs d'ex√©cution sont disponibles dans l'interface Airflow :
1. Cliquez sur le DAG
2. Cliquez sur une ex√©cution
3. Cliquez sur une t√¢che
4. Cliquez sur "Log"

## üéâ C'est tout !

Votre syst√®me est maintenant configur√© pour :
- ‚úÖ Scraper automatiquement tous les jours √† 10h
- ‚úÖ D√©tecter les nouvelles offres
- ‚úÖ Vous envoyer un email avec les nouvelles annonces

Bon courage dans votre recherche d'emploi ! üöÄ

